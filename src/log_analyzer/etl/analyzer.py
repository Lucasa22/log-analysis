"""
Log Analyzer - AnÃ¡lise de dados de logs
======================================

MÃ³dulo responsÃ¡vel por gerar mÃ©tricas e anÃ¡lises a partir de logs transformados.
"""

import logging
from typing import Dict, List, Optional, Any, Union
from datetime import datetime

from pyspark.sql import SparkSession, DataFrame
from pyspark.sql.functions import (
    col, count, desc, sum as spark_sum, max as spark_max,
    min as spark_min, avg, date_format, dayofweek, countDistinct
)

from log_analyzer.core.spark import get_spark_session
from log_analyzer.etl.load import load_to_database, load_to_parquet

logger = logging.getLogger(__name__)


def analyze_logs(
    df: DataFrame, 
    output_path: Optional[str] = None,
    save_to_db: bool = False,
    spark_session: Optional[SparkSession] = None
) -> Dict[str, DataFrame]:
    """
    Analisa os logs transformados e gera diversas mÃ©tricas.
    
    Args:
        df: DataFrame com os logs transformados
        output_path: Caminho opcional para salvar resultados em parquet
        save_to_db: Se True, salva resultados no banco de dados configurado
        spark_session: SessÃ£o Spark opcional
        
    Returns:
        DicionÃ¡rio com DataFrames de mÃ©tricas calculadas
    """
    spark = spark_session or get_spark_session()
    
    logger.info("ðŸ” Iniciando anÃ¡lise de logs...")
    start_time = datetime.now()
    
    # Registra o DataFrame como uma tabela temporÃ¡ria para consultas SQL
    df.createOrReplaceTempView("logs")
    
    # Calcula as vÃ¡rias mÃ©tricas
    metrics = {}
    metrics['summary'] = create_summary_metrics(df)
    metrics['top_ips'] = create_top_ips(df)
    metrics['top_endpoints'] = create_top_endpoints(df)
    metrics['error_analysis'] = create_error_analysis(df)
    metrics['peak_traffic'] = create_peak_traffic_analysis(df)
    
    processing_time = (datetime.now() - start_time).total_seconds()
    logger.info(f"âœ… AnÃ¡lise concluÃ­da em {processing_time:.2f} segundos")
    
    # Salva os resultados, se solicitado
    if output_path:
        logger.info(f"ðŸ’¾ Salvando resultados em {output_path}...")
        for name, metric_df in metrics.items():
            metric_path = f"{output_path}/{name}"
            load_to_parquet(metric_df, metric_path)
    
    # Salva no banco de dados, se solicitado
    if save_to_db:
        logger.info("ðŸ“Š Salvando mÃ©tricas no banco de dados...")
        from log_analyzer.core.config import get_settings
        
        settings = get_settings()
        db_config = settings.get("database", {})
        db_url = db_config.get("silver_db_url")
        db_properties = {
            "user": db_config.get("silver_db_user"),
            "password": db_config.get("silver_db_password"),
            "driver": db_config.get("silver_db_driver"),
        }
        
        for name, metric_df in metrics.items():
            try:
                table_name = db_config.get("gold_tables", {}).get(name, f"gold_{name}")
                load_to_database(metric_df, table_name, db_url, db_properties)
                logger.info(f"âœ… MÃ©trica '{name}' salva no banco de dados com sucesso")
            except Exception as e:
                logger.warning(f"âš ï¸ Erro ao salvar mÃ©trica '{name}' no banco de dados: {str(e)}")
                logger.info("â„¹ï¸ Continuando com as prÃ³ximas mÃ©tricas...")
    
    return metrics


def create_summary_metrics(df: DataFrame) -> DataFrame:
    """Cria mÃ©tricas de resumo do trÃ¡fego."""
    logger.info("ðŸ“Š Gerando mÃ©tricas de resumo...")
    
    return df.agg(
        count("*").alias("total_requests"),
        countDistinct("ip").alias("unique_visitors"),
        countDistinct("date").alias("days_span"),
        avg("bytes_sent_int").alias("avg_response_size"),
        spark_sum((col("status_int") >= 400).cast("int")).alias("total_errors"),
        (spark_sum((col("status_int") >= 400).cast("int")) / count("*") * 100).alias("error_rate")
    )


def create_top_ips(df: DataFrame, limit: int = 10) -> DataFrame:
    """Cria mÃ©tricas dos IPs com mais requisiÃ§Ãµes."""
    logger.info("ðŸ–¥ï¸ Identificando IPs com mais requisiÃ§Ãµes...")
    
    return df.groupBy("ip").agg(
        count("*").alias("requests"),
        spark_sum("bytes_sent_int").alias("total_bytes"),
        spark_sum((col("status_int") >= 400).cast("int")).alias("errors")
    ).orderBy(desc("requests")).limit(limit)


def create_top_endpoints(df: DataFrame, limit: int = 10) -> DataFrame:
    """Cria mÃ©tricas dos endpoints mais acessados."""
    logger.info("ðŸŒ Identificando endpoints mais acessados...")
    
    return (
        df.filter(~col("is_static_asset"))
        .groupBy("url")
        .agg(
            count("*").alias("requests"),
            avg("bytes_sent_int").alias("avg_size"),
            spark_sum((col("status_int") >= 400).cast("int")).alias("errors")
        )
        .orderBy(desc("requests"))
        .limit(limit)
    )


def create_error_analysis(df: DataFrame) -> DataFrame:
    """Analisa os erros por cÃ³digo de status."""
    logger.info("âŒ Analisando erros por cÃ³digo de status...")
    
    return (
        df.filter(col("status_int") >= 400)
        .groupBy("status", "status_int")
        .agg(
            count("*").alias("count"),
            countDistinct("ip").alias("affected_users"),
            countDistinct("date").alias("affected_days")
        )
        .orderBy(desc("count"))
    )


def create_peak_traffic_analysis(df: DataFrame) -> DataFrame:
    """Identifica perÃ­odos de pico de trÃ¡fego."""
    logger.info("ðŸ“ˆ Identificando perÃ­odos de pico de trÃ¡fego...")
    
    return (
        df.groupBy("date", "hour")
        .agg(
            count("*").alias("requests"),
            spark_sum("bytes_sent_int").alias("total_bytes"),
            spark_sum((col("status_int") >= 400).cast("int")).alias("errors")
        )
        .orderBy(desc("requests"))
    )
