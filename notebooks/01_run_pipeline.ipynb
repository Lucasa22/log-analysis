{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Log Analysis Pipeline - Notebook Test\n",
    "\n",
    "Este notebook testa o pipeline de an√°lise de logs localmente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-27T17:19:49.023637Z",
     "iopub.status.busy": "2025-06-27T17:19:49.023400Z",
     "iopub.status.idle": "2025-06-27T17:19:49.071971Z",
     "shell.execute_reply": "2025-06-27T17:19:49.071459Z"
    }
   },
   "outputs": [],
   "source": [
    "# Importar m√≥dulos necess√°rios\n",
    "import sys\n",
    "sys.path.append('../src')\n",
    "\n",
    "# Importa√ß√£o ultra simplificada - apenas um √∫nico import\n",
    "from log_analyzer.etl import run_pipeline\n",
    "from log_analyzer.core.spark import get_spark_session"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Framework de ETL\n",
    "O framework de ETL foi projetado para ser simples e modular, permitindo f√°cil integra√ß√£o e reutiliza√ß√£o. A estrutura do projeto √© a seguinte:\n",
    "\n",
    "```\n",
    "log_analyzer/\n",
    "  etl/\n",
    "    extractor.py   - Fun√ß√µes para extra√ß√£o de logs\n",
    "    transformer.py - Fun√ß√µes para transforma√ß√£o de dados\n",
    "    analyzer.py    - Fun√ß√µes para an√°lises e m√©tricas\n",
    "    load.py        - Fun√ß√µes para carregamento de dados\n",
    "    simple_pipeline.py - Pipeline simplificado que orquestra tudo\n",
    "```\n",
    "\n",
    "\n",
    "```python\n",
    "from log_analyzer.etl import run_pipeline, extract_logs, transform_logs, analyze_logs\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-27T17:19:49.183298Z",
     "iopub.status.busy": "2025-06-27T17:19:49.183162Z",
     "iopub.status.idle": "2025-06-27T17:19:54.756837Z",
     "shell.execute_reply": "2025-06-27T17:19:54.756500Z"
    }
   },
   "outputs": [],
   "source": [
    "# Verificar se o Spark est√° funcionando\n",
    "try:\n",
    "    spark = get_spark_session()\n",
    "    print(f\"Spark version: {spark.version}\")\n",
    "    print(f\"Spark UI: http://localhost:4040\")\n",
    "    print(f\"SparkContext ativo: {not spark._jsc.sc().isStopped()}\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è Erro ao inicializar o Spark: {str(e)}\")\n",
    "    print(\"Tentando criar uma nova sess√£o Spark...\")\n",
    "    from log_analyzer.core.spark import get_spark_session\n",
    "    spark = get_spark_session(app_name=\"log_analyzer_notebook_retry\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-27T17:19:54.758357Z",
     "iopub.status.busy": "2025-06-27T17:19:54.758250Z",
     "iopub.status.idle": "2025-06-27T17:20:23.127593Z",
     "shell.execute_reply": "2025-06-27T17:20:23.127165Z"
    }
   },
   "outputs": [],
   "source": [
    "# Executar o pipeline completo usando a API simplificada\n",
    "result = run_pipeline(\n",
    "    input_path=\"../data/logs.txt\",\n",
    "    output_path=\"../data\"\n",
    ")\n",
    "\n",
    "print(f\"Status: {result['status']}\")\n",
    "if result['status'] == 'success':\n",
    "    print(f\"‚úÖ Pipeline executado com sucesso!\")\n",
    "    print(f\"Registros processados: {result.get('processed_records', 0)}\")\n",
    "    print(f\"Output: {result.get('output_path', '../data')}\")\n",
    "    \n",
    "    # Exibir m√©tricas-chave\n",
    "    metrics = result.get(\"metrics\", {})\n",
    "    if metrics:\n",
    "        print(\"\\nüìä M√©tricas principais:\")\n",
    "        for key, value in metrics.items():\n",
    "            if not isinstance(value, (dict, list)):\n",
    "                print(f\"  - {key}: {value}\")\n",
    "else:\n",
    "    print(f\"‚ùå Falha no pipeline: {result.get('error', 'Erro desconhecido')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-27T17:20:23.129402Z",
     "iopub.status.busy": "2025-06-27T17:20:23.129274Z",
     "iopub.status.idle": "2025-06-27T17:20:23.140039Z",
     "shell.execute_reply": "2025-06-27T17:20:23.139798Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "from datetime import datetime, timedelta\n",
    "from pathlib import Path\n",
    "\n",
    "# Criar diret√≥rio para testes se n√£o existir\n",
    "test_dir = Path(\"../data/test\")\n",
    "test_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Caminho do arquivo de logs de teste\n",
    "test_log_path = test_dir / \"dummy_logs.txt\"\n",
    "\n",
    "# Lista de IPs de exemplo\n",
    "sample_ips = [\n",
    "    \"192.168.1.\" + str(i) for i in range(1, 20)\n",
    "] + [\"10.0.0.\" + str(i) for i in range(1, 10)]\n",
    "\n",
    "# URLs de exemplo \n",
    "sample_urls = [\n",
    "    \"/home\", \"/about\", \"/login\", \"/logout\", \"/dashboard\", \n",
    "    \"/profile\", \"/settings\", \"/api/data\", \"/static/main.css\", \n",
    "    \"/static/logo.png\", \"/static/app.js\"\n",
    "]\n",
    "\n",
    "# C√≥digos de status HTTP\n",
    "status_codes = [200, 200, 200, 200, 200, 301, 302, 404, 403, 500]\n",
    "\n",
    "# Formatos de User Agent\n",
    "user_agents = [\n",
    "    \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36\",\n",
    "    \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15\",\n",
    "    \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko)\",\n",
    "    \"Mozilla/5.0 (iPhone; CPU iPhone OS 14_7_1 like Mac OS X)\"\n",
    "]\n",
    "\n",
    "# Gerar logs aleat√≥rios no formato Apache Combined\n",
    "with open(test_log_path, \"w\") as f:\n",
    "    # Gerar 1000 linhas de logs\n",
    "    base_time = datetime.now() - timedelta(days=2)\n",
    "    \n",
    "    for i in range(1000):\n",
    "        ip = random.choice(sample_ips)\n",
    "        timestamp = base_time + timedelta(minutes=i//10)  # Incremento a cada 10 registros\n",
    "        formatted_time = timestamp.strftime(\"%d/%b/%Y:%H:%M:%S +0000\")\n",
    "        method = random.choice([\"GET\", \"POST\", \"PUT\", \"DELETE\"])\n",
    "        url = random.choice(sample_urls)\n",
    "        status = random.choice(status_codes)\n",
    "        size = random.randint(100, 10000) if status != 404 else 0\n",
    "        referer = \"https://example.com\" if random.random() > 0.5 else \"-\"\n",
    "        user_agent = random.choice(user_agents)\n",
    "        \n",
    "        log_line = f'{ip} - - [{formatted_time}] \"{method} {url} HTTP/1.1\" {status} {size} \"{referer}\" \"{user_agent}\"\\n'\n",
    "        f.write(log_line)\n",
    "\n",
    "print(f\"‚úÖ Arquivo de logs de teste criado em: {test_log_path}\")\n",
    "print(f\"Tamanho: {os.path.getsize(test_log_path)} bytes\")\n",
    "print(f\"Primeiras 3 linhas:\")\n",
    "\n",
    "# Mostrar algumas linhas do arquivo gerado\n",
    "with open(test_log_path, \"r\") as f:\n",
    "    for _ in range(3):\n",
    "        print(f.readline().strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-27T17:20:23.141447Z",
     "iopub.status.busy": "2025-06-27T17:20:23.141351Z",
     "iopub.status.idle": "2025-06-27T17:20:38.223807Z",
     "shell.execute_reply": "2025-06-27T17:20:38.223135Z"
    }
   },
   "outputs": [],
   "source": [
    "# Executar o pipeline com o arquivo de logs fict√≠cio\n",
    "test_output_path = test_dir / \"output\"\n",
    "\n",
    "print(f\"üöÄ Executando pipeline com os logs fict√≠cios...\")\n",
    "dummy_result = run_pipeline(\n",
    "    input_path=str(test_log_path),\n",
    "    output_path=str(test_output_path),\n",
    "    save_to_db=False\n",
    ")\n",
    "\n",
    "print(f\"\\nStatus: {dummy_result['status']}\")\n",
    "if dummy_result['status'] == 'success':\n",
    "    print(f\"‚úÖ Pipeline executado com sucesso!\")\n",
    "    print(f\"Registros processados: {dummy_result.get('processed_records', 0)}\")\n",
    "    print(f\"Output: {dummy_result.get('output_path', str(test_output_path))}\")\n",
    "    \n",
    "    # Exibir m√©tricas-chave\n",
    "    metrics = dummy_result.get(\"metrics\", {})\n",
    "    if metrics:\n",
    "        print(\"\\nüìä M√©tricas principais:\")\n",
    "        for key, value in metrics.items():\n",
    "            if not isinstance(value, (dict, list)):\n",
    "                print(f\"  - {key}: {value}\")\n",
    "else:\n",
    "    print(f\"‚ùå Falha no pipeline: {dummy_result.get('error', 'Erro desconhecido')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-27T17:20:38.225856Z",
     "iopub.status.busy": "2025-06-27T17:20:38.225605Z",
     "iopub.status.idle": "2025-06-27T17:20:38.232191Z",
     "shell.execute_reply": "2025-06-27T17:20:38.231926Z"
    }
   },
   "outputs": [],
   "source": [
    "# Analisar os resultados gerados com os logs fict√≠cios\n",
    "import json\n",
    "import os\n",
    "import glob\n",
    "\n",
    "# Verificar os diret√≥rios criados\n",
    "test_output_dir = test_output_path\n",
    "print(f\"\\nüìÇ Estrutura de diret√≥rios criada:\")\n",
    "for root, dirs, files in os.walk(test_output_dir):\n",
    "    level = root.replace(str(test_output_dir), '').count(os.sep)\n",
    "    indent = ' ' * 4 * level\n",
    "    print(f\"{indent}{os.path.basename(root)}/\")\n",
    "    sub_indent = ' ' * 4 * (level + 1)\n",
    "    for file in files:\n",
    "        print(f\"{sub_indent}{file}\")\n",
    "\n",
    "# Verificar se temos m√©tricas diretamente no resultado do pipeline\n",
    "if 'metrics' in dummy_result and dummy_result['metrics']:\n",
    "    print(f\"\\nüìä M√©tricas dispon√≠veis no resultado do pipeline:\")\n",
    "    for key, value in dummy_result['metrics'].items():\n",
    "        if isinstance(value, (dict, list)):\n",
    "            print(f\"  - {key}: (objeto complexo)\")\n",
    "        else:\n",
    "            print(f\"  - {key}: {value}\")\n",
    "else:\n",
    "    # Caso contr√°rio, tentar buscar m√©tricas em arquivos\n",
    "    gold_dir = test_output_dir / \"gold\"\n",
    "    if gold_dir.exists():\n",
    "        # Procurar por arquivos JSON com m√©tricas\n",
    "        json_files = glob.glob(f\"{gold_dir}/summary_metrics.json\")\n",
    "        if not json_files:\n",
    "            json_files = glob.glob(f\"{gold_dir}/*.json\")\n",
    "        \n",
    "        if json_files:\n",
    "            with open(json_files[0], 'r') as f:\n",
    "                try:\n",
    "                    metrics = json.load(f)\n",
    "                    print(f\"\\nüìä M√©tricas Geradas com Logs Fict√≠cios:\")\n",
    "                    if isinstance(metrics, dict):\n",
    "                        for key, value in metrics.items():\n",
    "                            print(f\"  - {key}: {value}\")\n",
    "                    else:\n",
    "                        print(metrics)\n",
    "                except json.JSONDecodeError:\n",
    "                    # Tenta ler como JSON lines (cada linha um JSON)\n",
    "                    f.seek(0)\n",
    "                    metrics = [json.loads(line) for line in f]\n",
    "                    print(f\"\\nüìä M√©tricas Geradas com Logs Fict√≠cios:\")\n",
    "                    for metric in metrics:\n",
    "                        print(f\"  - {metric.get('metric', 'unknown')}: {metric.get('value', 'N/A')}\")\n",
    "        else:\n",
    "            print(f\"\\nNenhum arquivo JSON encontrado em {gold_dir}\")\n",
    "    else:\n",
    "        print(f\"\\nDiret√≥rio Gold n√£o foi criado: {gold_dir}\")\n",
    "\n",
    "# Tratamento de erro para sess√£o Spark encerrada\n",
    "try:\n",
    "    spark\n",
    "except NameError:\n",
    "    print(\"\\n‚ö†Ô∏è Erro: A sess√£o Spark foi encerrada. Por favor, inicie uma nova sess√£o Spark.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-27T17:20:38.233627Z",
     "iopub.status.busy": "2025-06-27T17:20:38.233519Z",
     "iopub.status.idle": "2025-06-27T17:20:43.977773Z",
     "shell.execute_reply": "2025-06-27T17:20:43.977347Z"
    }
   },
   "outputs": [],
   "source": [
    "# Carregando e visualizando os dados processados dos logs fict√≠cios\n",
    "import traceback\n",
    "bronze_path = test_output_dir / \"bronze\" / \"logs.parquet\"\n",
    "silver_path = test_output_dir / \"silver\" / \"logs.parquet\"\n",
    "\n",
    "try:\n",
    "    # Verificar se o Spark est√° ativo\n",
    "    if spark._jsc.sc().isStopped():\n",
    "        print(\"‚ö†Ô∏è SparkContext foi encerrado. Criando uma nova sess√£o...\")\n",
    "        from log_analyzer.core.spark import get_spark_session\n",
    "        spark = get_spark_session()\n",
    "    \n",
    "    if bronze_path.parent.exists():\n",
    "        df_test_bronze = spark.read.parquet(str(bronze_path))\n",
    "        print(f\"\\nüì¶ Bronze Layer (Logs Fict√≠cios):\")\n",
    "        print(f\"Total de registros: {df_test_bronze.count()}\")\n",
    "        print(f\"Schema:\")\n",
    "        df_test_bronze.printSchema()\n",
    "        print(f\"Primeiros 5 registros:\")\n",
    "        df_test_bronze.show(5, truncate=False)\n",
    "    else:\n",
    "        print(f\"Bronze layer n√£o encontrada: {bronze_path}\")\n",
    "\n",
    "    if silver_path.parent.exists():\n",
    "        df_test_silver = spark.read.parquet(str(silver_path))\n",
    "        print(f\"\\nü•à Silver Layer (Logs Fict√≠cios):\")\n",
    "        print(f\"Total de registros: {df_test_silver.count()}\")\n",
    "        \n",
    "        # Analisar distribui√ß√£o de status HTTP\n",
    "        print(f\"\\nDistribui√ß√£o de c√≥digos de status HTTP:\")\n",
    "        df_test_silver.groupBy(\"status_int\").count().orderBy(\"status_int\").show()\n",
    "        \n",
    "        # Analisar top IPs\n",
    "        print(f\"\\nTop 5 IPs por n√∫mero de requisi√ß√µes:\")\n",
    "        from pyspark.sql.functions import count, desc\n",
    "        df_test_silver.groupBy(\"ip\").agg(count(\"*\").alias(\"requests\")).orderBy(desc(\"requests\")).limit(5).show()\n",
    "        \n",
    "        # Verificar URLs mais acessadas\n",
    "        print(f\"\\nTop 5 URLs mais acessadas:\")\n",
    "        df_test_silver.groupBy(\"url\").agg(count(\"*\").alias(\"requests\")).orderBy(desc(\"requests\")).limit(5).show(truncate=False)\n",
    "    else:\n",
    "        print(f\"Silver layer n√£o encontrada: {silver_path}\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è Erro ao processar dados Parquet: {str(e)}\")\n",
    "    print(\"Continuando com o restante do notebook...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-27T17:20:43.980771Z",
     "iopub.status.busy": "2025-06-27T17:20:43.980624Z",
     "iopub.status.idle": "2025-06-27T17:21:01.240914Z",
     "shell.execute_reply": "2025-06-27T17:21:01.240590Z"
    }
   },
   "outputs": [],
   "source": [
    "# Ler e exibir as m√©tricas finais\n",
    "import json\n",
    "import os\n",
    "import glob\n",
    "from pathlib import Path\n",
    "\n",
    "# Reimplementamos o pipeline, ent√£o vamos execut√°-lo novamente para ter os dados mais recentes\n",
    "print(\"üîÑ Executando o pipeline novamente para obter as m√©tricas mais recentes...\")\n",
    "fresh_result = run_pipeline(\n",
    "    input_path=\"../data/logs.txt\",\n",
    "    output_path=\"../data\"\n",
    ")\n",
    "\n",
    "if fresh_result[\"status\"] == \"success\" and \"metrics\" in fresh_result:\n",
    "    print(\"\\nüìä M√©tricas do resultado direto do pipeline:\")\n",
    "    metrics = fresh_result[\"metrics\"]\n",
    "    \n",
    "    # Exibir m√©tricas estruturadas\n",
    "    for key, value in metrics.items():\n",
    "        if isinstance(value, dict):\n",
    "            print(f\"\\n  üìà {key}:\")\n",
    "            for sub_key, sub_value in value.items():\n",
    "                print(f\"    - {sub_key}: {sub_value}\")\n",
    "        elif isinstance(value, list):\n",
    "            print(f\"\\n  üìà {key} (top 5):\")\n",
    "            for i, item in enumerate(value[:5]):\n",
    "                if isinstance(item, dict):\n",
    "                    print(f\"    {i+1}. {item}\")\n",
    "                else:\n",
    "                    print(f\"    {i+1}. {item}\")\n",
    "        else:\n",
    "            print(f\"  - {key}: {value}\")\n",
    "else:\n",
    "    # M√©todo alternativo: procurar por m√©tricas na pasta gold\n",
    "    print(\"\\nBuscando m√©tricas em arquivos JSON...\")\n",
    "    gold_dir = Path(\"../data/gold\")\n",
    "    json_files = glob.glob(f\"{gold_dir}/summary_metrics.json\")\n",
    "\n",
    "    if not json_files:\n",
    "        json_files = glob.glob(f\"{gold_dir}/*.json\")\n",
    "\n",
    "    if json_files:\n",
    "        with open(json_files[0], 'r') as f:\n",
    "            try:\n",
    "                # Tentar primeiro como um √∫nico JSON\n",
    "                metrics = json.load(f)\n",
    "                print(\"\\nüìä M√©tricas Geradas:\")\n",
    "                for key, value in metrics.items():\n",
    "                    print(f\"  - {key}: {value}\")\n",
    "            except json.JSONDecodeError:\n",
    "                # Se falhar, tentar como JSON lines (cada linha um objeto)\n",
    "                f.seek(0)  # Voltar ao in√≠cio do arquivo\n",
    "                metrics = [json.loads(line) for line in f]\n",
    "                print(\"\\nüìä M√©tricas Geradas (formato JSON lines):\")\n",
    "                for metric in metrics:\n",
    "                    print(f\"  - {metric.get('metric', 'unknown')}: {metric.get('value', 'N/A')}\")\n",
    "    else:\n",
    "        print(\"Nenhuma m√©trica JSON encontrada na pasta Gold!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-27T17:21:01.242517Z",
     "iopub.status.busy": "2025-06-27T17:21:01.242408Z",
     "iopub.status.idle": "2025-06-27T17:21:03.937988Z",
     "shell.execute_reply": "2025-06-27T17:21:03.937698Z"
    }
   },
   "outputs": [],
   "source": [
    "# Visualizar dados Bronze\n",
    "try:\n",
    "    # Verificar se o Spark est√° ativo\n",
    "    if spark._jsc.sc().isStopped():\n",
    "        print(\"‚ö†Ô∏è SparkContext foi encerrado. Criando uma nova sess√£o...\")\n",
    "        from log_analyzer.core.spark import get_spark_session\n",
    "        spark = get_spark_session()\n",
    "        \n",
    "    df_bronze = spark.read.parquet(\"../data/bronze/logs.parquet\")\n",
    "    print(f\"\\nüì¶ Bronze Layer:\")\n",
    "    print(f\"Total de registros: {df_bronze.count()}\")\n",
    "    print(f\"\\nSchema:\")\n",
    "    df_bronze.printSchema()\n",
    "    print(f\"\\nPrimeiros 5 registros:\")\n",
    "    df_bronze.show(5, truncate=False)\n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è Erro ao processar dados Bronze: {str(e)}\")\n",
    "    print(\"Continuando com o restante do notebook...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-27T17:21:03.939574Z",
     "iopub.status.busy": "2025-06-27T17:21:03.939461Z",
     "iopub.status.idle": "2025-06-27T17:21:05.129935Z",
     "shell.execute_reply": "2025-06-27T17:21:05.129505Z"
    }
   },
   "outputs": [],
   "source": [
    "# Visualizar dados Silver\n",
    "try:\n",
    "    # Verificar se o Spark est√° ativo\n",
    "    if spark._jsc.sc().isStopped():\n",
    "        print(\"‚ö†Ô∏è SparkContext foi encerrado. Criando uma nova sess√£o...\")\n",
    "        from log_analyzer.core.spark import get_spark_session\n",
    "        spark = get_spark_session()\n",
    "        \n",
    "    # Tentar reutilizar a vari√°vel df_bronze se existir\n",
    "    if 'df_bronze' not in locals() or df_bronze is None:\n",
    "        try:\n",
    "            df_bronze = spark.read.parquet(\"../data/bronze/logs.parquet\")\n",
    "        except:\n",
    "            df_bronze = None\n",
    "            print(\"N√£o foi poss√≠vel carregar os dados Bronze para compara√ß√£o\")\n",
    "    \n",
    "    df_silver = spark.read.parquet(\"../data/silver/logs.parquet\")\n",
    "    print(f\"\\nü•à Silver Layer:\")\n",
    "    print(f\"Total de registros: {df_silver.count()}\")\n",
    "    \n",
    "    if df_bronze is not None:\n",
    "        print(f\"\\nCampos adicionados na transforma√ß√£o:\")\n",
    "        silver_cols = set(df_silver.columns)\n",
    "        bronze_cols = set(df_bronze.columns)\n",
    "        new_cols = silver_cols - bronze_cols\n",
    "        print(f\"Novos campos: {new_cols}\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è Erro ao processar dados Silver: {str(e)}\")\n",
    "    print(\"Continuando com o restante do notebook...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-27T17:21:05.131722Z",
     "iopub.status.busy": "2025-06-27T17:21:05.131593Z",
     "iopub.status.idle": "2025-06-27T17:21:06.192005Z",
     "shell.execute_reply": "2025-06-27T17:21:06.191359Z"
    }
   },
   "outputs": [],
   "source": [
    "# An√°lise adicional - Top 5 IPs\n",
    "try:\n",
    "    # Verificar se o Spark est√° ativo\n",
    "    if spark._jsc.sc().isStopped():\n",
    "        print(\"‚ö†Ô∏è SparkContext foi encerrado. Criando uma nova sess√£o...\")\n",
    "        from log_analyzer.core.spark import get_spark_session\n",
    "        spark = get_spark_session()\n",
    "        \n",
    "    # Se a vari√°vel df_silver n√£o existir ou for None, recarreg√°-la\n",
    "    if 'df_silver' not in locals() or df_silver is None:\n",
    "        df_silver = spark.read.parquet(\"../data/silver/logs.parquet\")\n",
    "    \n",
    "    from pyspark.sql.functions import count, desc\n",
    "    top_ips = df_silver.groupBy(\"ip\").agg(count(\"*\").alias(\"requests\")).orderBy(desc(\"requests\")).limit(5)\n",
    "    print(\"\\nüéØ Top 5 IPs por n√∫mero de requisi√ß√µes:\")\n",
    "    top_ips.show()\n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è Erro ao analisar Top 5 IPs: {str(e)}\")\n",
    "    print(\"Continuando com o restante do notebook...\")\n",
    "    # Criar uma vari√°vel vazia para evitar erros nas c√©lulas subsequentes\n",
    "    top_ips = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-27T17:21:06.193741Z",
     "iopub.status.busy": "2025-06-27T17:21:06.193618Z",
     "iopub.status.idle": "2025-06-27T17:21:06.196422Z",
     "shell.execute_reply": "2025-06-27T17:21:06.195610Z"
    }
   },
   "outputs": [],
   "source": [
    "# N√£o fechar a sess√£o Spark aqui, vamos mov√™-la para o final\n",
    "# spark.stop()\n",
    "print(\"‚úÖ Mantendo a sess√£o Spark ativa para o restante do notebook\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-27T17:21:06.199342Z",
     "iopub.status.busy": "2025-06-27T17:21:06.199160Z",
     "iopub.status.idle": "2025-06-27T17:21:06.202738Z",
     "shell.execute_reply": "2025-06-27T17:21:06.201775Z"
    }
   },
   "outputs": [],
   "source": [
    "# Exemplo de uso das fun√ß√µes individuais do pipeline\n",
    "from log_analyzer.etl import extract_logs, transform_logs, analyze_logs, load_to_parquet\n",
    "\n",
    "print(\"\\nüöÄ Demonstra√ß√£o de uso das fun√ß√µes individuais do pipeline:\")\n",
    "print(\"Este exemplo mostra como voc√™ pode usar cada componente separadamente\")\n",
    "print(\"ou combin√°-los da forma que precisar\")\n",
    "\n",
    "# Exemplo simplificado - n√£o executa de verdade para n√£o duplicar o processamento\n",
    "print(\"\\nEtapas individuais:\")\n",
    "print(\"1. df_bronze = extract_logs('arquivo_logs.txt', log_format='common')\")\n",
    "print(\"2. df_silver = transform_logs(df_bronze)\")\n",
    "print(\"3. resultados = analyze_logs(df_silver)\")\n",
    "print(\"4. load_to_parquet(df_silver, 'caminho/saida')\")\n",
    "\n",
    "print(\"\\n‚úÖ Com esta abordagem modular, voc√™ pode facilmente:\")\n",
    "print(\"- Reimplementar apenas partes espec√≠ficas do pipeline\")\n",
    "print(\"- Combinar componentes de forma flex√≠vel\")\n",
    "print(\"- Testar cada etapa separadamente\")\n",
    "print(\"- Entender melhor o fluxo de dados\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-27T17:21:06.204942Z",
     "iopub.status.busy": "2025-06-27T17:21:06.204830Z",
     "iopub.status.idle": "2025-06-27T17:21:07.631674Z",
     "shell.execute_reply": "2025-06-27T17:21:07.631060Z"
    }
   },
   "outputs": [],
   "source": [
    "# Exemplo de uso direto da API simplificada\n",
    "try:\n",
    "    from log_analyzer.etl.load import load_to_parquet\n",
    "    \n",
    "    # Verificar se temos a vari√°vel top_ips dispon√≠vel\n",
    "    if 'top_ips' not in locals() or top_ips is None:\n",
    "        print(\"‚ö†Ô∏è Dados de top_ips n√£o est√£o dispon√≠veis. Criando dados de exemplo...\")\n",
    "        \n",
    "        # Verificar se o Spark est√° ativo\n",
    "        if spark._jsc.sc().isStopped():\n",
    "            print(\"‚ö†Ô∏è SparkContext foi encerrado. Criando uma nova sess√£o...\")\n",
    "            from log_analyzer.core.spark import get_spark_session\n",
    "            spark = get_spark_session()\n",
    "        \n",
    "        # Criar um DataFrame de exemplo simples\n",
    "        data = [(\"192.168.1.1\", 100), (\"192.168.1.2\", 80), (\"192.168.1.3\", 50)]\n",
    "        top_ips = spark.createDataFrame(data, [\"ip\", \"requests\"])\n",
    "    \n",
    "    # Vamos salvar o resultado da an√°lise dos top IPs em um formato simples\n",
    "    output_path = \"../data/examples/top_ips_example\"\n",
    "    print(f\"\\nüí° Demonstra√ß√£o da API simplificada de carregamento:\")\n",
    "    print(f\"Salvando top IPs em: {output_path}\")\n",
    "\n",
    "    # Uso direto da fun√ß√£o load_to_parquet sem precisar instanciar classes\n",
    "    load_to_parquet(\n",
    "        df=top_ips,\n",
    "        output_path=output_path,\n",
    "        mode=\"overwrite\",\n",
    "        partition_by=None\n",
    "    )\n",
    "\n",
    "    print(\"‚úÖ Dados salvos com sucesso usando a API simplificada!\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è Erro ao demonstrar carregamento de dados: {str(e)}\")\n",
    "    print(\"Finalizando o notebook...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-27T17:21:07.634508Z",
     "iopub.status.busy": "2025-06-27T17:21:07.634205Z",
     "iopub.status.idle": "2025-06-27T17:21:12.896022Z",
     "shell.execute_reply": "2025-06-27T17:21:12.895149Z"
    }
   },
   "outputs": [],
   "source": [
    "# Demonstra√ß√£o do problema de tipo em PySpark e sua solu√ß√£o\n",
    "try:\n",
    "    # Verificar se o Spark est√° ativo\n",
    "    if 'spark' not in locals() or spark._jsc.sc().isStopped():\n",
    "        print(\"Criando nova sess√£o Spark para demonstra√ß√£o...\")\n",
    "        from log_analyzer.core.spark import get_spark_session\n",
    "        spark = get_spark_session(app_name=\"boolean_cast_demo\")\n",
    "    \n",
    "    # Criar um DataFrame simples para demonstra√ß√£o\n",
    "    from pyspark.sql import Row\n",
    "    from pyspark.sql.functions import col, sum as spark_sum\n",
    "    \n",
    "    # Dados de exemplo: status HTTP\n",
    "    data = [\n",
    "        Row(url=\"/home\", status=200),\n",
    "        Row(url=\"/about\", status=200),\n",
    "        Row(url=\"/not-found\", status=404),\n",
    "        Row(url=\"/login\", status=200),\n",
    "        Row(url=\"/admin\", status=403),\n",
    "        Row(url=\"/error\", status=500)\n",
    "    ]\n",
    "    \n",
    "    df = spark.createDataFrame(data)\n",
    "    \n",
    "    print(\"DataFrame de demonstra√ß√£o:\")\n",
    "    df.show()\n",
    "    \n",
    "    # Demonstra√ß√£o do problema\n",
    "    print(\"\\nCriando uma express√£o booleana: status >= 400\")\n",
    "    df.withColumn(\"is_error\", col(\"status\") >= 400).show()\n",
    "    \n",
    "    print(\"\\n1) Contando erros corretamente (com cast para integer):\")\n",
    "    result = df.agg(spark_sum((col(\"status\") >= 400).cast(\"int\")).alias(\"total_errors\"))\n",
    "    result.show()\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è Erro na demonstra√ß√£o: {str(e)}\")\n",
    "    print(\"Continuando com o notebook...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-27T17:21:12.897757Z",
     "iopub.status.busy": "2025-06-27T17:21:12.897644Z",
     "iopub.status.idle": "2025-06-27T17:21:13.291255Z",
     "shell.execute_reply": "2025-06-27T17:21:13.290992Z"
    }
   },
   "outputs": [],
   "source": [
    "spark.stop()\n",
    "print(\"‚úÖ Sess√£o Spark encerrada com sucesso\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
